{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "To read more about the ingest pipeline, checkout the docs [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html).\n",
    "\n",
    "![ingest_pipelines_docs](../images/ingest_pipelines_docs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to ElasticSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  name: \"fead23d3120d\",\n",
      "  cluster_name: \"docker-cluster\",\n",
      "  cluster_uuid: \"v3fUyW9OReext6IjPiOCqg\",\n",
      "  version: {\n",
      "    number: \"8.17.4\",\n",
      "    build_flavor: \"default\",\n",
      "    build_type: \"docker\",\n",
      "    build_hash: \"c63c7f5f8ce7d2e4805b7b3d842e7e792d84dda1\",\n",
      "    build_date: \"2025-03-20T15:39:59.811110136Z\",\n",
      "    build_snapshot: false,\n",
      "    lucene_version: \"9.12.0\",\n",
      "    minimum_wire_compatibility_version: \"7.17.0\",\n",
      "    minimum_index_compatibility_version: \"7.0.0\"\n",
      "  },\n",
      "  tagline: \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { Client } from \"npm:@elastic/elasticsearch\";\n",
    "import { load } from \"https://deno.land/std/dotenv/mod.ts\";\n",
    "\n",
    "const env = await load({ envPath: \"../.env\" });\n",
    "\n",
    "const client = new Client({\n",
    "  node: env.ELASTICSEARCH_NODE,\n",
    "  auth: {\n",
    "    apiKey: env.ELASTICSEARCH_API_KEY,\n",
    "  },\n",
    "});\n",
    "\n",
    "const info = await client.info();\n",
    "console.log(info);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ acknowledged: \u001b[33mtrue\u001b[39m }"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.ingest.putPipeline({\n",
    "  id: \"lowercase_pipeline\",\n",
    "  description: \"Transform text to lowercase\",\n",
    "  processors: [\n",
    "    {\n",
    "      lowercase: {\n",
    "        field: \"text\",\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  lowercase_pipeline: {\n",
       "    description: \u001b[32m\"Transform text to lowercase\"\u001b[39m,\n",
       "    processors: [ { lowercase: { field: \u001b[32m\"text\"\u001b[39m } } ]\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.ingest.getPipeline({\n",
    "  id: \"lowercase_pipeline\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ acknowledged: \u001b[33mtrue\u001b[39m }"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.ingest.deletePipeline({\n",
    "  id: \"lowercase_pipeline\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulate method allows you to give the pipeline fake data just to test if it is working or not. This is usually done before applying the pipeline to your real index and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the docs list, we are providing some test data. After executing the cell, nothing will be indexed. You will just get back how the documents will look like after the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  docs: [\n",
       "    {\n",
       "      doc: {\n",
       "        _index: \u001b[32m\"my_index\"\u001b[39m,\n",
       "        _version: \u001b[32m\"-3\"\u001b[39m,\n",
       "        _id: \u001b[32m\"1\"\u001b[39m,\n",
       "        _source: { text: \u001b[32m\"hello world\"\u001b[39m },\n",
       "        _ingest: { timestamp: \u001b[32m\"2025-04-09T11:12:43.632259486Z\"\u001b[39m }\n",
       "      }\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.ingest.simulate({\n",
    "  id: \"lowercase_pipeline\",\n",
    "  docs: [\n",
    "    {\n",
    "      _index: \"my_index\",\n",
    "      _id: \"1\",\n",
    "      _source: {\n",
    "        text: \"Hello World\",\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data and make the text uppercased to see if the `lowercase_pipeline` will be executed before indexing the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    title: \"Title 1\",\n",
      "    text: \"THIS IS THE FIRST SAMPLE DOCUMENT TEXT.\",\n",
      "    createdAt: \"2025-03-01\"\n",
      "  },\n",
      "  {\n",
      "    title: \"Title 2\",\n",
      "    text: \"HERE IS ANOTHER EXAMPLE OF A DOCUMENT.\",\n",
      "    createdAt: \"2025-03-02\"\n",
      "  },\n",
      "  {\n",
      "    title: \"Title 3\",\n",
      "    text: \"THE CONTENT OF THE THIRD DOCUMENT GOES HERE.\",\n",
      "    createdAt: \"2025-03-03\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import data from \"../data/dummy_data.json\" with { type: \"json\" };\n",
    "\n",
    "for (const document of data) {\n",
    "  document.text = document.text.toUpperCase();\n",
    "}\n",
    "\n",
    "console.log(data);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ acknowledged: \u001b[33mtrue\u001b[39m, shards_acknowledged: \u001b[33mtrue\u001b[39m, index: \u001b[32m\"my_index\"\u001b[39m }"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.indices.delete({ index: \"my_index\", ignore_unavailable: true });\n",
    "await client.indices.create({ index: \"my_index\" });\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we pass the `lowercase_pipeline` to the bulk method. It will perform the transformations before indexing the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  errors: \u001b[33mfalse\u001b[39m,\n",
       "  took: \u001b[33m400\u001b[39m,\n",
       "  ingest_took: \u001b[33m0\u001b[39m,\n",
       "  items: [\n",
       "    {\n",
       "      index: {\n",
       "        _index: \u001b[32m\"my_index\"\u001b[39m,\n",
       "        _id: \u001b[32m\"bIlTGpYBkrY7cs0FuRKv\"\u001b[39m,\n",
       "        _version: \u001b[33m1\u001b[39m,\n",
       "        result: \u001b[32m\"created\"\u001b[39m,\n",
       "        _shards: { total: \u001b[33m2\u001b[39m, successful: \u001b[33m1\u001b[39m, failed: \u001b[33m0\u001b[39m },\n",
       "        _seq_no: \u001b[33m0\u001b[39m,\n",
       "        _primary_term: \u001b[33m1\u001b[39m,\n",
       "        status: \u001b[33m201\u001b[39m\n",
       "      }\n",
       "    },\n",
       "    {\n",
       "      index: {\n",
       "        _index: \u001b[32m\"my_index\"\u001b[39m,\n",
       "        _id: \u001b[32m\"bYlTGpYBkrY7cs0FuRKv\"\u001b[39m,\n",
       "        _version: \u001b[33m1\u001b[39m,\n",
       "        result: \u001b[32m\"created\"\u001b[39m,\n",
       "        _shards: { total: \u001b[33m2\u001b[39m, successful: \u001b[33m1\u001b[39m, failed: \u001b[33m0\u001b[39m },\n",
       "        _seq_no: \u001b[33m1\u001b[39m,\n",
       "        _primary_term: \u001b[33m1\u001b[39m,\n",
       "        status: \u001b[33m201\u001b[39m\n",
       "      }\n",
       "    },\n",
       "    {\n",
       "      index: {\n",
       "        _index: \u001b[32m\"my_index\"\u001b[39m,\n",
       "        _id: \u001b[32m\"bolTGpYBkrY7cs0FuRKv\"\u001b[39m,\n",
       "        _version: \u001b[33m1\u001b[39m,\n",
       "        result: \u001b[32m\"created\"\u001b[39m,\n",
       "        _shards: { total: \u001b[33m2\u001b[39m, successful: \u001b[33m1\u001b[39m, failed: \u001b[33m0\u001b[39m },\n",
       "        _seq_no: \u001b[33m2\u001b[39m,\n",
       "        _primary_term: \u001b[33m1\u001b[39m,\n",
       "        status: \u001b[33m201\u001b[39m\n",
       "      }\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const operations = [];\n",
    "for (const document of data) {\n",
    "  operations.push({\n",
    "    index: {\n",
    "      _index: \"my_index\",\n",
    "    },\n",
    "  });\n",
    "  operations.push(document);\n",
    "}\n",
    "\n",
    "await client.bulk({\n",
    "  operations,\n",
    "  pipeline: \"lowercase_pipeline\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After indexing the documents, we can see that the `text` field for all documents has been lowercased. This indicates the pipeline did run with no issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  title: \"Title 1\",\n",
      "  createdAt: \"2025-03-01\",\n",
      "  text: \"this is the first sample document text.\"\n",
      "}\n",
      "{\n",
      "  title: \"Title 2\",\n",
      "  createdAt: \"2025-03-02\",\n",
      "  text: \"here is another example of a document.\"\n",
      "}\n",
      "{\n",
      "  title: \"Title 3\",\n",
      "  createdAt: \"2025-03-03\",\n",
      "  text: \"the content of the third document goes here.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const response = await client.search({\n",
    "  index: \"my_index\",\n",
    "});\n",
    "\n",
    "const hits = response.hits.hits;\n",
    "for (const hit of hits) {\n",
    "  console.log(hit._source);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Not handling the failure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, we don’t handle failures with `ignore_failure` or `on_failure`. Instead, the pipeline will raise an exception, halting execution of any further processes, and the document will not be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ acknowledged: \u001b[33mtrue\u001b[39m }"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.ingest.putPipeline({\n",
    "  id: \"my_pipeline\",\n",
    "  description: \"Pipeline with multiple transformations\",\n",
    "  processors: [\n",
    "    {\n",
    "      lowercase: {\n",
    "        field: \"text\",\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      set: {\n",
    "        field: \"text\",\n",
    "        value: \"CHANGED BY PIPELINE\",\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "illegal_argument_exception\n\tRoot causes:\n\t\tillegal_argument_exception: field [text] not present as part of path [text]",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "ResponseError: illegal_argument_exception",
      "\tRoot causes:",
      "\t\tillegal_argument_exception: field [text] not present as part of path [text]",
      "    at SniffingTransport._request (file:///C:/Users/PC/AppData/Local/deno/npm/registry.npmjs.org/@elastic/transport/8.9.5/lib/Transport.js:543:27)",
      "    at Object.runMicrotasks (ext:core/01_core.js:692:26)",
      "    at processTicksAndRejections (ext:deno_node/_next_tick.ts:59:10)",
      "    at runNextTicks (ext:deno_node/_next_tick.ts:76:3)",
      "    at eventLoopTick (ext:core/01_core.js:185:21)",
      "    at async file:///C:/Users/PC/AppData/Local/deno/npm/registry.npmjs.org/@elastic/transport/8.9.5/lib/Transport.js:641:32",
      "    at async SniffingTransport.request (file:///C:/Users/PC/AppData/Local/deno/npm/registry.npmjs.org/@elastic/transport/8.9.5/lib/Transport.js:637:20)",
      "    at async Client.IndexApi [as index] (file:///C:/Users/PC/AppData/Local/deno/npm/registry.npmjs.org/@elastic/elasticsearch/8.17.1/lib/api/api/index.js:59:12)",
      "    at async <anonymous>:5:1"
     ]
    }
   ],
   "source": [
    "const document = {\n",
    "  title: \"Hello World\",\n",
    "  createdAt: \"2025-04-01\",\n",
    "};\n",
    "\n",
    "await client.index({\n",
    "  index: \"my_index\",\n",
    "  pipeline: \"my_pipeline\",\n",
    "  document,\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Handling the failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the failures, we use `ignore_failure` or define an `on_failure` block. With `ignore_failure`, the pipeline will skip over the failed step and continue executing subsequent processes without interrupting the flow, allowing other documents to be indexed.\n",
    "\n",
    "Alternatively, with `on_failure`, we can specify custom error-handling steps, such as logging the error, retrying, or sending notifications, ensuring the pipeline proceeds even if one step encounters an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ acknowledged: \u001b[33mtrue\u001b[39m }"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.ingest.putPipeline({\n",
    "  id: \"my_pipeline_2\",\n",
    "  description: \"Pipeline with multiple transformations\",\n",
    "  processors: [\n",
    "    {\n",
    "      lowercase: {\n",
    "        field: \"text\",\n",
    "        on_failure: [\n",
    "          {\n",
    "            set: {\n",
    "              field: \"text\",\n",
    "              value: \"FAILED TO LOWERCASE\",\n",
    "              ignore_failure: true,\n",
    "            },\n",
    "          },\n",
    "        ],\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      set: {\n",
    "        field: \"text\",\n",
    "        value: \"ADDED BY PIPELINE\",\n",
    "        ignore_failure: true,\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  _index: \u001b[32m\"my_index\"\u001b[39m,\n",
       "  _id: \u001b[32m\"b4lUGpYBkrY7cs0FGxLI\"\u001b[39m,\n",
       "  _version: \u001b[33m1\u001b[39m,\n",
       "  result: \u001b[32m\"created\"\u001b[39m,\n",
       "  _shards: { total: \u001b[33m2\u001b[39m, successful: \u001b[33m1\u001b[39m, failed: \u001b[33m0\u001b[39m },\n",
       "  _seq_no: \u001b[33m3\u001b[39m,\n",
       "  _primary_term: \u001b[33m1\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const document = {\n",
    "  title: \"Hello World\",\n",
    "  createdAt: \"2025-04-01\",\n",
    "};\n",
    "\n",
    "await client.index({\n",
    "  index: \"my_index\",\n",
    "  pipeline: \"my_pipeline_2\",\n",
    "  document,\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  title: \"Title 1\",\n",
      "  createdAt: \"2025-03-01\",\n",
      "  text: \"this is the first sample document text.\"\n",
      "}\n",
      "{\n",
      "  title: \"Title 2\",\n",
      "  createdAt: \"2025-03-02\",\n",
      "  text: \"here is another example of a document.\"\n",
      "}\n",
      "{\n",
      "  title: \"Title 3\",\n",
      "  createdAt: \"2025-03-03\",\n",
      "  text: \"the content of the third document goes here.\"\n",
      "}\n",
      "{\n",
      "  title: \"Hello World\",\n",
      "  createdAt: \"2025-04-01\",\n",
      "  text: \"ADDED BY PIPELINE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const response = await client.search({\n",
    "  index: \"my_index\",\n",
    "});\n",
    "\n",
    "const hits = response.hits.hits;\n",
    "for (const hit of hits) {\n",
    "  console.log(hit._source);\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
